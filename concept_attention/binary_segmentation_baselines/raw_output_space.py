"""
    This baseline just returns heatmaps as the raw cross attentions.
"""
from concept_attention.flux.src.flux.sampling import prepare, unpack
import torch
import einops
import PIL

from concept_attention.image_generator import FluxGenerator
from concept_attention.segmentation import SegmentationAbstractClass, add_noise_to_image, encode_image
from concept_attention.utils import embed_concepts, linear_normalization

def compute_heatmaps_from_vectors(
    image_vectors,
    concept_vectors,
    layer_indices: list[int],
    timesteps: list[int] = list(range(4)),
    softmax: bool = True,
    normalize_concepts: bool = False
):
    """
        Accepts image vectors and concept vectors. These can be from cross attentions or attention outputs.  
    """
    # Check if there are heads in the input 
    if len(image_vectors.shape) == 6: 
        # Collapse the had dimension
        image_vectors = einops.rearrange(
            image_vectors,
            "time layers batch head patches dim -> time layers batch patches (head dim)"
        )
        concept_vectors = einops.rearrange(
            concept_vectors,
            "time layers batch head concepts dim -> time layers batch concepts (head dim)"
        )


    # Apply linear normalization to concepts
    if normalize_concepts:
        concept_vectors = linear_normalization(concept_vectors, dim=-2)

    # Compute heatmaps 
    heatmaps = einops.einsum(
        image_vectors, 
        concept_vectors,
        "time batch patches dim, time batch concepts dim -> time batch concepts patches",
    )

    # Apply softmax
    if softmax:
        heatmaps = torch.nn.functional.softmax(heatmaps, dim=-2)
    # Pull out the timesteps and layers
    heatmaps = heatmaps[timesteps]
    # Average over the heatmaps
    heatmaps = einops.reduce(
        heatmaps,
        "time batch concepts patches -> batch concepts patches",
        reduction="mean"
    )
    heatmaps = einops.rearrange(
        heatmaps,
        "batch concepts (h w) -> batch concepts h w",
        h=64,
        w=64
    )

    return heatmaps


class RawOutputSpaceBaseline():
    """
        This class implements the cross attention baseline. 
    """

    def __init__(
        self,
        model_name: str = "flux-schnell",
        device: str = "cuda",
        offload: bool = True,
        generator: FluxGenerator = None
    ):
        """
            Initialize the DAAM model.
        """
        super(RawOutputSpaceBaseline, self).__init__()
        if generator is None:
            # Load up the flux generator
            self.generator = FluxGenerator(
                model_name=model_name,
                device=device,
                offload=offload,
            )
        else:
            self.generator = generator
        # Unpack the tokenizer
        self.tokenizer = self.generator.t5.tokenizer

    def __call__(
        self,
        prompt,
        concepts,
        seed=4,
        num_steps=4,
        timesteps: list[int] = list(range(2, 4)),
        layers: list[int] = list(range(10, 19)),
        softmax=False
    ):
        """
            Generate cross attention heatmap visualizations. 

            Args:
            - prompt: str, the prompt to generate the visualizations for
            - seed: int, the seed to use for the visualization

            Returns:
            - attention_maps: torch.Tensor, the attention maps for the prompt
            - tokens: list[str], the tokens in the prompt
            - image: torch.Tensor, the image generated by the
        """
        # Run the image generator
        image, _, concept_attention_maps = self.generator.generate_image(
            width=1024,
            height=1024,
            num_steps=num_steps,
            seed=seed,
            prompt=prompt,
            concepts=concepts,
            guidance=0.0
        )
        # Do softmax
        if softmax:
            concept_attention_maps = torch.nn.functional.softmax(concept_attention_maps, dim=-2)
        # Pull out the desired layers
        concept_attention_maps = concept_attention_maps[:, layers]
        # Pull out the desired timesteps
        concept_attention_maps = concept_attention_maps[timesteps]
        # AVerage over the layers, time heads
        concept_attention_maps = einops.reduce(
            concept_attention_maps,
            "layers time heads concepts patches -> concepts patches",
            reduction="mean"
        )
        # Rearrange
        concept_attention_maps = einops.rearrange(
            concept_attention_maps,
            "concepts (h w) -> concepts h w",
            h=64,
            w=64
        )

        return concept_attention_maps, image

class RawOutputSpaceSegmentationModel(SegmentationAbstractClass):

    def __init__(
        self,
        model_name: str = "flux-schnell",
        device: str = "cuda",
        offload: bool = True,
        generator=None,
    ):
        """
            Initialize the segmentation model.
        """
        super(RawOutputSpaceSegmentationModel, self).__init__()
        if generator is not None:
            self.generator = generator
        else:
            # Load up the flux generator
            self.generator = FluxGenerator(
                model_name=model_name,
                device=device,
                offload=offload,
            )

        self.is_schnell = "schnell" in model_name

    def segment_individual_image(
        self,
        image: PIL.Image.Image,
        concepts: list[str],
        caption: str,
        device: str = "cuda",
        offload: bool = False,
        num_samples: int = 1,
        num_steps: int = 4,
        noise_timestep: int = 2,
        seed: int = 4,
        width: int = 1024,
        height: int = 1024,
        stop_after_multimodal_attentions: bool = True,
        layers: list[int] = list(range(19)),
        layer_indices = list(range(15, 19)),
        normalize_concepts=True,
        softmax: bool = False,
        num_inference_steps: int = 4,
        timesteps = None,
        joint_attention_kwargs=None,
        **kwargs
    ):
        """
            Takes a real image and generates a segmentation map. 
        """

        if timesteps is None:
            timesteps = list(range(num_inference_steps))
        
        # Encode the image into the VAE latent space
        encoded_image_without_noise = encode_image(
            image,
            self.generator.ae,
            offload=offload,
            device=device,
        )
        # Do N trials
        
        all_image_vectors = []
        all_concept_vectors = []
        for i in range(num_samples):
            # Add noise to image
            encoded_image, timesteps = add_noise_to_image(
                encoded_image_without_noise,
                num_steps=num_steps,
                noise_timestep=noise_timestep,
                seed=seed + i,
                width=width,
                height=height,
                device=device,
                is_schnell=self.is_schnell,
            )
            # Now run the diffusion model once on the noisy image
            # Encode the concept vectors
            
            if offload:
                self.generator.t5, self.generator.clip = self.generator.t5.to(device), self.generator.clip.to(device)
            inp = prepare(t5=self.generator.t5, clip=self.generator.clip, img=encoded_image, prompt=caption)

            concept_embeddings, concept_ids, concept_vec = embed_concepts(
                self.generator.clip,
                self.generator.t5,
                concepts,
            )

            inp["concepts"] = concept_embeddings.to(encoded_image.device)
            inp["concept_ids"] = concept_ids.to(encoded_image.device)
            inp["concept_vec"] = concept_vec.to(encoded_image.device)
            # offload TEs to CPU, load model to gpu
            if offload:
                self.generator.t5, self.generator.clip = self.generator.t5.cpu(), self.generator.clip.cpu()
                torch.cuda.empty_cache()
                self.generator.model = self.generator.model.to(device)
            # Denoise the intermediate images
            guidance_vec = torch.full((encoded_image.shape[0],), 0.0, device=encoded_image.device, dtype=encoded_image.dtype)
            t_curr = timesteps[0]
            t_prev = timesteps[1]
            t_vec = torch.full((encoded_image.shape[0],), t_curr, dtype=encoded_image.dtype, device=encoded_image.device)
            _, concept_heatmaps = self.generator.model(
                img=inp["img"],
                img_ids=inp["img_ids"],
                txt=inp["txt"],
                txt_ids=inp["txt_ids"],
                concepts=inp["concepts"],
                concept_ids=inp["concept_ids"],
                concept_vec=inp["concept_vec"],
                y=inp["concept_vec"],
                timesteps=t_vec,
                guidance=guidance_vec,
                stop_after_multimodal_attentions=stop_after_multimodal_attentions,
                joint_attention_kwargs=joint_attention_kwargs,
            )


            # all_concept_heatmaps.append(concept_heatmaps["cross_attention_image_vectors"])
            # all_concept_heatmaps.append(concept_heatmaps["cross_attention_concept_vectors"])
            all_image_vectors.append(concept_heatmaps["output_space_image_vectors"])
            all_concept_vectors.append(concept_heatmaps["output_space_concept_vectors"])
            # all_concept_heatmaps.append(concept_heatmaps["output_space_concept_vectors"])


        # Stack and average across samples
        all_image_vectors = torch.stack(all_image_vectors, dim=0)  # [samples, time, layers, batch, patches, dim]
        all_concept_vectors = torch.stack(all_concept_vectors, dim=0)

        # Average across samples
        image_vectors = einops.reduce(all_image_vectors, "samples time batch patches dim -> time batch patches dim", reduction="mean")
        concept_vectors = einops.reduce(all_concept_vectors, "samples time batch concepts dim -> time batch concepts dim", reduction="mean")

        # Compute heatmaps
        concept_heatmaps = compute_heatmaps_from_vectors(
            image_vectors,
            concept_vectors,
            layer_indices=layer_indices,
            timesteps=timesteps,  
            softmax=softmax,
            normalize_concepts=normalize_concepts
        )

        if not stop_after_multimodal_attentions:
            img = inp["img"] + (t_prev - t_curr) * pred
            # decode latents to pixel space
            img = unpack(img.float(), height, width)
            with torch.autocast(device_type=self.generator.device.type, dtype=torch.bfloat16):
                img = self.generator.ae.decode(img)

            if self.generator.offload:
                self.generator.ae.decoder.cpu()
                torch.cuda.empty_cache()
            img = img.clamp(-1, 1)
            img = einops.rearrange(img[0], "c h w -> h w c")
            # reconstructed_image = PIL.Image.fromarray(img.cpu().byte().numpy())
            reconstructed_image = PIL.Image.fromarray((127.5 * (img + 1.0)).cpu().byte().numpy())
        else:
            reconstructed_image = None
        

        return concept_heatmaps, reconstructed_image